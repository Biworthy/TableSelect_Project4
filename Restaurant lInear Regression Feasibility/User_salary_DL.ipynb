{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-23T06:16:28.845013Z",
     "start_time": "2022-05-23T06:16:24.900536Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.9.1-cp38-cp38-win_amd64.whl (444.1 MB)\n",
      "Collecting tensorflow-estimator<2.10.0,>=2.9.0rc0\n",
      "  Downloading tensorflow_estimator-2.9.0-py2.py3-none-any.whl (438 kB)\n",
      "Collecting termcolor>=1.1.0\n",
      "  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (2.10.0)\n",
      "Collecting keras<2.10.0,>=2.9.0rc0\n",
      "  Downloading keras-2.9.0-py2.py3-none-any.whl (1.6 MB)\n",
      "Collecting tensorboard<2.10,>=2.9\n",
      "  Downloading tensorboard-2.9.0-py3-none-any.whl (5.8 MB)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (4.2.0)\n",
      "Collecting libclang>=13.0.0\n",
      "  Downloading libclang-14.0.1-py2.py3-none-win_amd64.whl (14.2 MB)\n",
      "Collecting gast<=0.4.0,>=0.2.1\n",
      "  Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
      "Collecting google-pasta>=0.1.1\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Collecting protobuf<3.20,>=3.9.2\n",
      "  Downloading protobuf-3.19.4-cp38-cp38-win_amd64.whl (895 kB)\n",
      "Collecting numpy>=1.20\n",
      "  Downloading numpy-1.22.4-cp38-cp38-win_amd64.whl (14.8 MB)\n",
      "Requirement already satisfied: setuptools in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (49.2.0.post20200714)\n",
      "Requirement already satisfied: packaging in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (20.4)\n",
      "Collecting opt-einsum>=2.3.2\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "Collecting grpcio<2.0,>=1.24.3\n",
      "  Downloading grpcio-1.46.3-cp38-cp38-win_amd64.whl (3.5 MB)\n",
      "Collecting absl-py>=1.0.0\n",
      "  Downloading absl_py-1.0.0-py3-none-any.whl (126 kB)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.26.0-cp38-cp38-win_amd64.whl (1.5 MB)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (1.15.0)\n",
      "Collecting keras-preprocessing>=1.1.1"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not install packages due to an EnvironmentError: [WinError 5] Access is denied: 'c:\\\\programdata\\\\anaconda3\\\\lib\\\\site-packages\\\\numpy\\\\compat\\\\py3k.py'\n",
      "Consider using the `--user` option or check the permissions.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (1.11.2)\n",
      "Collecting flatbuffers<2,>=1.12\n",
      "  Downloading flatbuffers-1.12-py2.py3-none-any.whl (15 kB)\n",
      "Collecting astunparse>=1.6.0\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard<2.10,>=2.9->tensorflow) (2.6.2)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard<2.10,>=2.9->tensorflow) (2.24.0)\n",
      "Collecting tensorboard-data-server<0.7.0,>=0.6.0\n",
      "  Downloading tensorboard_data_server-0.6.1-py3-none-any.whl (2.4 kB)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard<2.10,>=2.9->tensorflow) (1.0.1)\n",
      "Collecting tensorboard-plugin-wit>=1.6.0\n",
      "  Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
      "Requirement already satisfied: wheel>=0.26 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard<2.10,>=2.9->tensorflow) (0.34.2)\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
      "Collecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.3.7-py3-none-any.whl (97 kB)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from packaging->tensorflow) (2.4.7)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in c:\\programdata\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (4.8)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (5.0.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (2020.6.20)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (1.25.11)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Collecting importlib-metadata>=4.4; python_version < \"3.10\"\n",
      "  Downloading importlib_metadata-4.11.4-py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\programdata\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (0.4.8)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Downloading oauthlib-3.2.0-py3-none-any.whl (151 kB)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from importlib-metadata>=4.4; python_version < \"3.10\"->markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow) (3.1.0)\n",
      "Building wheels for collected packages: termcolor\n",
      "  Building wheel for termcolor (setup.py): started\n",
      "  Building wheel for termcolor (setup.py): finished with status 'done'\n",
      "  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4835 sha256=33f3b45381f8ee66025005e9c7f779674228f77f5b01c1c0c0542daa976573e5\n",
      "  Stored in directory: c:\\users\\hem\\appdata\\local\\pip\\cache\\wheels\\a0\\16\\9c\\5473df82468f958445479c59e784896fa24f4a5fc024b0f501\n",
      "Successfully built termcolor\n",
      "Installing collected packages: tensorflow-estimator, termcolor, keras, tensorboard-data-server, tensorboard-plugin-wit, oauthlib, requests-oauthlib, google-auth-oauthlib, absl-py, grpcio, protobuf, numpy, importlib-metadata, markdown, tensorboard, libclang, gast, google-pasta, opt-einsum, tensorflow-io-gcs-filesystem, keras-preprocessing, flatbuffers, astunparse, tensorflow\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 3.20.0\n",
      "    Uninstalling protobuf-3.20.0:\n",
      "      Successfully uninstalled protobuf-3.20.0\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.18.5\n",
      "    Uninstalling numpy-1.18.5:\n",
      "Requirement already satisfied: googledrivedownloader in c:\\programdata\\anaconda3\\lib\\site-packages (0.4)\n",
      "Requirement already satisfied: plotly in c:\\programdata\\anaconda3\\lib\\site-packages (5.8.0)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from plotly) (8.0.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow\n",
    "!pip install googledrivedownloader\n",
    "!pip install plotly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "\n",
    "Note: This notebook uses `python 3` and these packages: `tensorflow`, `pandas`, `matplotlib`, `scikit-learn`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries & Helper Functions\n",
    "\n",
    "First of all, we will need to import some libraries and helper functions. This includes TensorFlow and some utility functions that I've written to save time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPORT THE NECESSARY LIBRARIES\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objs as go\n",
    "import plotly.offline as py\n",
    "import seaborn as sns\n",
    "\n",
    "import matplotlib.ticker as mtick\n",
    "plt.style.use('fivethirtyeight')\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import  ExtraTreesRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from utils import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping, LambdaCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-23T06:16:28.860983Z",
     "start_time": "2022-05-23T06:16:28.845982Z"
    }
   },
   "outputs": [],
   "source": [
    "#!ls /content/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Importing the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is saved in a `base_merged_2.csv` file. We will use `pandas` to take a look at some of the rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-23T06:16:28.876984Z",
     "start_time": "2022-05-23T06:16:28.863008Z"
    }
   },
   "outputs": [],
   "source": [
    "from google_drive_downloader import GoogleDriveDownloader as gdd\n",
    "file_name = './base_merged_2.csv'\n",
    "gdd.download_file_from_google_drive(file_id='1rYw4OQ-uK3YJohQmekV-Drio2wlEHA9N',\n",
    "                                    dest_path=file_name,\n",
    "                                    unzip=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-23T06:16:28.908012Z",
     "start_time": "2022-05-23T06:16:28.877982Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-23T06:16:28.924013Z",
     "start_time": "2022-05-23T06:16:28.908982Z"
    }
   },
   "outputs": [],
   "source": [
    "data.dtypes #checking the data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-23T06:16:28.939981Z",
     "start_time": "2022-05-23T06:16:28.924983Z"
    }
   },
   "outputs": [],
   "source": [
    "data.isna().sum() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-23T06:16:28.956013Z",
     "start_time": "2022-05-23T06:16:28.940982Z"
    }
   },
   "outputs": [],
   "source": [
    "#Deleting Unnnecessary Columns\n",
    "df=data.drop(['MTW Status','TaxPaidperReturn','TaxPaidperReturn','IncomeperReturn'],axis=1) #Dropping the column like \"phone\" and \"url\" and saving the new dataset as \"df\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-23T06:16:37.565088Z",
     "start_time": "2022-05-23T06:16:28.956982Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#You can use pandas profiling to get an over all overview of the dataset\n",
    "import pandas_profiling as pf\n",
    "\n",
    "pf.ProfileReport(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Checking for duplicate values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-23T06:16:37.571090Z",
     "start_time": "2022-05-23T06:16:37.566090Z"
    }
   },
   "outputs": [],
   "source": [
    "df.duplicated().sum() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-23T06:16:37.586730Z",
     "start_time": "2022-05-23T06:16:37.571090Z"
    }
   },
   "outputs": [],
   "source": [
    "df.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-23T06:16:37.602354Z",
     "start_time": "2022-05-23T06:16:37.586730Z"
    }
   },
   "outputs": [],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-21T07:03:26.715932Z",
     "start_time": "2022-05-21T07:03:26.699933Z"
    }
   },
   "source": [
    "**Drop null values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-23T06:16:37.617981Z",
     "start_time": "2022-05-23T06:16:37.602354Z"
    }
   },
   "outputs": [],
   "source": [
    "#Remove the NaN values from the dataset\n",
    "df.dropna(how='any',inplace=True)\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Renaming columes appropriately**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-23T06:16:37.633284Z",
     "start_time": "2022-05-23T06:16:37.617981Z"
    }
   },
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-23T06:16:37.649309Z",
     "start_time": "2022-05-23T06:16:37.634286Z"
    }
   },
   "outputs": [],
   "source": [
    "df = df.rename(columns={'Price_Rng':'Price_Range',\n",
    "                        'SalaryperReturn':'User_salary', \n",
    "                        'Ratings':'Review_rating' , \n",
    "                        'Reviews':'Total_reviews',\n",
    "                        'NumberReturns':'Population', \n",
    "                        'Zip-Code':'Zip_code', \n",
    "                        'AdjustedGross Income':'Gross_income'})\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-23T06:16:37.664558Z",
     "start_time": "2022-05-23T06:16:37.650285Z"
    }
   },
   "outputs": [],
   "source": [
    "df.Cuisine.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-23T06:16:37.681560Z",
     "start_time": "2022-05-23T06:16:37.665557Z"
    }
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cleaning the dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-23T06:16:37.712557Z",
     "start_time": "2022-05-23T06:16:37.686562Z"
    }
   },
   "outputs": [],
   "source": [
    "df['User_salary'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**replacing the \",\" and \"$\" with nothing and converting the results to float**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-23T06:16:37.728557Z",
     "start_time": "2022-05-23T06:16:37.713561Z"
    }
   },
   "outputs": [],
   "source": [
    "df['User_salary'] = df['User_salary'].apply(lambda x: x.replace('$','')) #Using lambda function to replace '$' from User_salary\n",
    "df['User_salary'] = df['User_salary'].apply(lambda x: x.replace(',','')) #Using lambda function to replace ',' from User_salary\n",
    "df['User_salary'] = df['User_salary'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-23T06:16:37.744557Z",
     "start_time": "2022-05-23T06:16:37.729559Z"
    }
   },
   "outputs": [],
   "source": [
    "df['Population'] = df['Population'].apply(lambda x: x.replace(',','')) #Using lambda function to replace ',' from Population\n",
    "df['Population'] = df['Population'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-23T06:16:37.760557Z",
     "start_time": "2022-05-23T06:16:37.745559Z"
    }
   },
   "outputs": [],
   "source": [
    "df['Gross_income'] = df['Gross_income'].apply(lambda x: x.replace(',','')) #Using lambda function to replace ',' from Gross_income\n",
    "df['Gross_income'] = df['Gross_income'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-23T06:16:37.776556Z",
     "start_time": "2022-05-23T06:16:37.761559Z"
    }
   },
   "outputs": [],
   "source": [
    "df['Name'] = df['Name'].apply(lambda x: x.replace(\"'\",'')) #Using lambda function to replace ',' from Gross_income"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-23T06:16:37.792556Z",
     "start_time": "2022-05-23T06:16:37.777559Z"
    }
   },
   "outputs": [],
   "source": [
    "print(df['User_salary'].unique())\n",
    "print('---'*10)\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-23T06:16:37.808588Z",
     "start_time": "2022-05-23T06:16:37.793559Z"
    }
   },
   "outputs": [],
   "source": [
    "df['Review_rating'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualisations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-23T06:16:38.016587Z",
     "start_time": "2022-05-23T06:16:37.809561Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(17,10))\n",
    "chains=df['Name'].value_counts()[:20]\n",
    "sns.barplot(x=chains,y=chains.index,palette='deep')\n",
    "plt.title(\"Most famous restaurants\")\n",
    "plt.xlabel(\"Number of outlets\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Rating Distributions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-23T06:16:38.970081Z",
     "start_time": "2022-05-23T06:16:38.017559Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9,7))\n",
    "sns.distplot(df['Review_rating'],bins=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*  **Insight**\n",
    "\n",
    "  We can infer from above that most of the ratings are within 3.5 and 4.5\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-23T06:16:39.127308Z",
     "start_time": "2022-05-23T06:16:38.970081Z"
    }
   },
   "outputs": [],
   "source": [
    "#Distribution of the cost Vs ratings in parallel with online order\n",
    "plt.figure(figsize=(5,4))\n",
    "sns.scatterplot(x=\"Review_rating\",y='Total_reviews',hue='Price_Range',data=df)\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-23T06:16:39.926572Z",
     "start_time": "2022-05-23T06:16:39.128308Z"
    }
   },
   "outputs": [],
   "source": [
    "#Distribution of the cost Vs ratings in parallel with online order\n",
    "plt.figure(figsize=(10,25))\n",
    "sns.scatterplot(x=\"Price_Range\",y='Cuisine',hue='Review_rating',data=df)\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-21T07:27:27.285965Z",
     "start_time": "2022-05-21T07:27:27.274965Z"
    }
   },
   "source": [
    "## **Cuisines Types**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-23T06:16:40.992884Z",
     "start_time": "2022-05-23T06:16:39.927572Z"
    }
   },
   "outputs": [],
   "source": [
    "#Types of Cuisine\n",
    "\n",
    "sns.countplot(df['Cuisine']).set_xticklabels(sns.countplot(df['Cuisine']).get_xticklabels(), rotation=90, ha=\"right\")\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(20,12)\n",
    "plt.title('Cuisine')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-23T06:16:41.399544Z",
     "start_time": "2022-05-23T06:16:40.993889Z"
    }
   },
   "outputs": [],
   "source": [
    "trace0=go.Box(y=df['Price_Range'],name=\"Accepting Price_Range\",\n",
    "              marker = dict(\n",
    "        color = 'rgb(113, 10, 100)',\n",
    "    ))\n",
    "data=[trace0]\n",
    "layout=go.Layout(title=\"Box plot of Approximate Price Range\",width=800,height=800,yaxis=dict(title=\"Price_Range\"))\n",
    "fig=go.Figure(data=data,layout=layout)\n",
    "py.iplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-23T06:16:41.692953Z",
     "start_time": "2022-05-23T06:16:41.400546Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,8))\n",
    "sns.distplot(df['Price_Range'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-23T06:16:41.993198Z",
     "start_time": "2022-05-23T06:16:41.694954Z"
    }
   },
   "outputs": [],
   "source": [
    "sns.heatmap(df.corr(),annot = True) #Just to see the correlation between the features and the label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Building Our Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-23T06:16:42.008229Z",
     "start_time": "2022-05-23T06:16:41.994201Z"
    }
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-23T06:16:42.024200Z",
     "start_time": "2022-05-23T06:16:42.009202Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "\n",
    "\n",
    "# Name              object\n",
    "# Cuisine           object\n",
    "# Review_rating    float64\n",
    "# Total_reviews      int64\n",
    "# Price_Range        int64\n",
    "# Zip_code           int64\n",
    "# State             object\n",
    "# City              object\n",
    "# Population       float64\n",
    "# Gross_income     float64\n",
    "# User_salary      float64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-23T06:16:42.040200Z",
     "start_time": "2022-05-23T06:16:42.025204Z"
    }
   },
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-23T06:16:42.247199Z",
     "start_time": "2022-05-23T06:16:42.041204Z"
    }
   },
   "outputs": [],
   "source": [
    "# Correlation matrix for all characteristics\n",
    "corrMat = df.corr()\n",
    "\n",
    "# Determine which correlations are significant, and drop the others\n",
    "# Assuming correlation values >= 0.8 are significant\n",
    "corrValues = corrMat.unstack().abs()\n",
    "c = [corrValues.drop(i, inplace = True) for i, v in corrValues.items()\n",
    "     if i[0] == i[1]                           # left and right index are the same\n",
    "     or v < 0.8                                # Value is insignificant\n",
    "     or (i[1], i[0]) in corrValues.index]      # repeated correlation values\n",
    "\n",
    "# Sort and print the correlation values\n",
    "print('Characteristics with significant correlations:\\n{}\\n'.format(corrValues.sort_values(ascending = False)))\n",
    "\n",
    "# Print the figure\n",
    "sns.set(font_scale = 1.0, rc = {'figure.figsize': (12, 10)})\n",
    "sns.heatmap(corrMat, vmax = 0.8, square = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-23T06:16:42.263201Z",
     "start_time": "2022-05-23T06:16:42.248205Z"
    }
   },
   "outputs": [],
   "source": [
    "# df = df.iloc[:,1:] \n",
    "# df_norm = (df - df.mean()) / df.std() #Data is normalized by subtracting each value in the column with the mean value and then dividing it with the standard                                                 deviation of the whole column\n",
    "# df_norm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-23T06:16:42.278200Z",
     "start_time": "2022-05-23T06:16:42.264201Z"
    }
   },
   "outputs": [],
   "source": [
    "# Name              object\n",
    "# Cuisine            int32\n",
    "# Review_rating    float64\n",
    "# Total_reviews      int64\n",
    "# Price_Range        int64\n",
    "# Zip_code           int64\n",
    "# State             object\n",
    "# City              object\n",
    "# Population       float64\n",
    "# Gross_income     float64\n",
    "# User_salary      float64\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-23T06:16:42.294224Z",
     "start_time": "2022-05-23T06:16:42.279203Z"
    }
   },
   "outputs": [],
   "source": [
    "df.Cuisine = le.fit_transform(df.Cuisine) # direct number convert \n",
    "\n",
    "\n",
    "# Converting the Cuisine into numerical_values using get_dummies method\n",
    "dummy_cols = pd.get_dummies(df.Cuisine)\n",
    "df_1 = pd.concat([df,dummy_cols], axis='columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-23T06:16:42.326234Z",
     "start_time": "2022-05-23T06:16:42.299202Z"
    }
   },
   "outputs": [],
   "source": [
    "df_1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-23T06:16:42.357229Z",
     "start_time": "2022-05-23T06:16:42.327202Z"
    }
   },
   "outputs": [],
   "source": [
    "# own_data = df.iloc[:,[2,3,4,5,6,9,10,11]]\n",
    "own_data = df_1.drop([ \"User_salary\",\"Name\",\"Cuisine\",\"State\",\"City\",\"id\"], axis='columns')\n",
    "own_data.to_csv('own_data.csv') \n",
    "own_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-23T06:16:42.373226Z",
     "start_time": "2022-05-23T06:16:42.358201Z"
    }
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-23T06:16:42.389236Z",
     "start_time": "2022-05-23T06:16:42.374202Z"
    }
   },
   "outputs": [],
   "source": [
    "df = df.drop([\"Gross_income\" ,\"Name\",\"Total_reviews\", \"State\",\"City\",\"id\"], axis='columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-23T06:16:42.405202Z",
     "start_time": "2022-05-23T06:16:42.390201Z"
    }
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-23T06:16:42.421199Z",
     "start_time": "2022-05-23T06:16:42.406202Z"
    }
   },
   "outputs": [],
   "source": [
    "# X1 = df.drop([\"Gross_income\" ,'User_salary',\"Name\",\"Total_reviews\", \"Cuisine\",\"State\",\"City\",\"id\"], axis='columns')\n",
    "# y1 = df['User_salary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-23T06:16:42.437229Z",
     "start_time": "2022-05-23T06:16:42.422201Z"
    }
   },
   "outputs": [],
   "source": [
    "# X = df_1.drop([\"Gross_income\" ,'User_salary',\"Name\",\"Total_reviews\", \"Cuisine\",\"State\",\"City\",\"id\"], axis='columns')\n",
    "# y = df_1['User_salary']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can make it easier for optimization algorithms to find minimas by normalizing the data before training a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-23T06:16:42.453231Z",
     "start_time": "2022-05-23T06:16:42.438198Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "df_norm = (df - df.mean()) / df.std() #Data is normalized by subtracting each value in the column with the mean value and then dividing it with the standard                                                 deviation of the whole column\n",
    "df_norm.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(convert_label_value(_______), \" that corresponds to the User_salary _______\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Training and Test Sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select Features\n",
    "\n",
    "Make sure to remove the column User_salary from the list of features as it is the label and should not be used as a feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-23T06:16:42.469201Z",
     "start_time": "2022-05-23T06:16:42.454198Z"
    }
   },
   "outputs": [],
   "source": [
    "X = df_norm.iloc[:, :5] #Storing the features in 'X'\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Select Labels\n",
    "We select the User_salary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-23T06:16:42.485201Z",
     "start_time": "2022-05-23T06:16:42.470201Z"
    }
   },
   "outputs": [],
   "source": [
    "Y = df_norm.iloc[:, -1] #Storing the labels in 'Y'\n",
    "Y.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Feature and Label Values\n",
    "\n",
    "We will need to extract just the numeric values for the features and labels as the TensorFlow model will expect just numeric values as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-23T06:16:42.501202Z",
     "start_time": "2022-05-23T06:16:42.486203Z"
    }
   },
   "outputs": [],
   "source": [
    "X_arr = X.values\n",
    "Y_arr = Y.values\n",
    "\n",
    "print('X_arr shape: ', X_arr.shape) #'shape' gives the dimension of the entity\n",
    "print('Y_arr shape: ', Y_arr.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Test Split\n",
    "\n",
    "We will keep some part of the data aside as a __test__ set. The model will not use this set during training and it will be used only for checking the performance of the model in trained and un-trained states. This way, we can make sure that we are going in the right direction with our model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-23T06:16:42.516198Z",
     "start_time": "2022-05-23T06:16:42.502204Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_arr, Y_arr, test_size = 0.05, shuffle = True, random_state=0) \n",
    "#This predefined function splits the dataset to train and test set, where test size is given in 'test_size'(Here 5%) \n",
    "#Random state ensures that the splits that you generate are reproducible. Scikit-learn uses random permutations to generate the splits.\n",
    "\n",
    "print('X_train shape: ', X_train.shape)\n",
    "print('y_train shape: ', y_train.shape)\n",
    "print('X_test shape: ', X_test.shape)\n",
    "print('y_test shape: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Convert Label Value\n",
    "\n",
    "Because we are using normalized values for the labels, we will get the predictions back from a trained model in the same distribution. So, we need to convert the predicted values back to the original distribution if we want predicted User_salares."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-23T06:16:42.532203Z",
     "start_time": "2022-05-23T06:16:42.517203Z"
    }
   },
   "outputs": [],
   "source": [
    "y_mean = df['User_salary'].mean()\n",
    "y_std = df['User_salary'].std()\n",
    "\n",
    "def convert_label_value(pred):       #Defining a function which will convert the label values back to the original distribution and return it\n",
    "    return int(pred * y_std + y_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Create the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A function that returns an untrained model of a certain architecture. We're using a simple neural network architecture with just three hidden layers. We're going to use the relu activation function on all\n",
    "the layers except for the output layer.\n",
    "\n",
    "\n",
    "We know that input shape is simply a list of 5 values because we just have 5 features. \n",
    "\n",
    "The activation is going to be really or rectified linear unit, and the next layer will be again a fully connected or dense\n",
    "layer. \n",
    "\n",
    "And this time, let's use 20 nodes and again,  the same activation function relu and one more hidden layer with 5.\n",
    "nodes and finally, the output layer with just 1 node. so we have essentially, we have three hidden\n",
    "layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create the Model\n",
    "\n",
    "Let's write a function that returns an untrained model of a certain architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-23T06:16:42.610199Z",
     "start_time": "2022-05-23T06:16:42.533200Z"
    }
   },
   "outputs": [],
   "source": [
    "from utils import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping, LambdaCallback\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "def get_model():\n",
    "    \n",
    "    model = Sequential([\n",
    "        Dense(10, input_shape = (5,), activation = 'relu'), #10 neurons, Input Layer\n",
    "        Dense(20, activation = 'relu'),                     #20 neurons, Hidden Layer\n",
    "        Dense(5, activation = 'relu'),                      #5  neurons, Hidden Layer\n",
    "        Dense(1)                                            #Output Layer\n",
    "    ])                                                      #'relu' activation\n",
    "\n",
    "    model.compile(\n",
    "        loss='mse',                                         #Trained using Mean square error loss (Cost function) \n",
    "        optimizer='adam',                                    #Optimizer used is 'adam' (One of the Fastest optimizers)\n",
    "        metrics=[keras.metrics.SparseCategoricalAccuracy()]\n",
    "    )\n",
    "    \n",
    "#     model.compile(\n",
    "#         optimizer='sgd',\n",
    "#         loss='binary_crossentropy',\n",
    "#         metrics=['accuracy'])\n",
    "    \n",
    "    \n",
    "    return model\n",
    "\n",
    "model = get_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the input and then we have three hidden layers.\n",
    "but 10, 20 and five nodes respectively.\n",
    "All the layers have activation function set to value\n",
    "except for the output layer.\n",
    "Since this is a regression problem, we just need the linear\n",
    "output without any activation function here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first dense layer that you see here is our first\n",
    "hidden there, which has 10 nodes.\n",
    "\n",
    "The next one has 20 nodes.\n",
    "Next one has 5 nodes.\n",
    "And the final one node, the output there has one, and you can see\n",
    "that we have trainable parameters count 401.\n",
    "\n",
    "Because these are dense layers, they are fully\n",
    "connected layers and if you want to understand how\n",
    "these parameters,count is arrived at, you can simply\n",
    "multiply the nodes in your output layer in in any\n",
    "one of these layers with the notes in the proceeding here.\n",
    "\n",
    "So if you if you take a look at dense to for example,\n",
    "we have 5 nodes and in the preceding layer that\n",
    "is connected to we have 20 nodes, so each no disconnected.\n",
    "\n",
    "\n",
    "Each node of dense one is connected to each note of dense two,\n",
    "which means we have total 100 connections.\n",
    "But you see that you have 105  parameters\n",
    "for the Slayer.\n",
    "Why is that?\n",
    "That's simply because even though we have 100 weights, we\n",
    "also have a bias or intercept,  in every layer.\n",
    "\n",
    "Now that is just one interceptor connected to all the nodes\n",
    "of the layer that you calculating these parameters for so\n",
    "520 gives you 100 and five into one gives you 5,\n",
    "and the total is 105 and you can do this exercise for all\n",
    "the layers and arrive at the same number of trainable\n",
    "parameters.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-23T06:16:42.625197Z",
     "start_time": "2022-05-23T06:16:42.611199Z"
    }
   },
   "outputs": [],
   "source": [
    "# early_stopping = EarlyStopping(monitor='val_loss', patience = 100) #Defining early stopping parameter (optional, to save time)\n",
    "\n",
    "# model = get_model()\n",
    "\n",
    "# preds_on_untrained = model.predict(X_test) #Make predictions on the test set before training the parameters\n",
    "\n",
    "# #Finally training the model-->\n",
    "# history = model.fit(\n",
    "#     X_train, y_train,\n",
    "#     validation_data = (X_test, y_test),\n",
    "#     epochs = 1000,\n",
    "#     callbacks = [early_stopping]\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "We can use an `EarlyStopping` callback from Keras to stop the model training if the validation loss stops decreasing for a few epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-05-23T18:27:24.797Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(monitor='val_loss', patience = 10) #Defining early stopping parameter (optional, to save time)\n",
    "\n",
    "model = get_model()\n",
    "\n",
    "preds_on_untrained = model.predict(X_test) #Make predictions on the test set before training the parameters\n",
    "\n",
    "#Finally training the model-->\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data = (X_test, y_test),\n",
    "    epochs = 6,\n",
    "    callbacks = [early_stopping]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Plot Training and Validation Loss\n",
    "\n",
    "Let's use the `plot_loss` helper function to take a look training and validation loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-23T06:16:43.853223Z",
     "start_time": "2022-05-23T06:16:43.712227Z"
    }
   },
   "outputs": [],
   "source": [
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "The training and validation loss.\n",
    "Values decreased as the training then don, and that's great.\n",
    "But we don't know yet if the train model actually makes\n",
    "reasonably accurate predictions.\n",
    "So let's take a look at that.\n",
    "Now. Remember that we had some predictions on the untrained\n",
    "model. Similarly, we will make some predictions on the train\n",
    "model on the same data set, of course, which is X test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-23T06:16:43.869224Z",
     "start_time": "2022-05-23T06:16:43.854225Z"
    }
   },
   "outputs": [],
   "source": [
    "print('X_test shape: ', X_test.shape)\n",
    "print('y_test shape: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-23T06:16:43.933248Z",
     "start_time": "2022-05-23T06:16:43.870226Z"
    }
   },
   "outputs": [],
   "source": [
    "loss = model.evaluate(X_test, y_test)\n",
    "print(\"Loss is : \",loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-23T06:16:44.076224Z",
     "start_time": "2022-05-23T06:16:43.934224Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_history(history):\n",
    "    loss_list = [s for s in history.history.keys() if 'loss' in s and 'val' not in s]\n",
    "    val_loss_list = [s for s in history.history.keys() if 'loss' in s and 'val' in s]\n",
    "    acc_list = [s for s in history.history.keys() if 'acc' in s and 'val' not in s]\n",
    "    val_acc_list = [s for s in history.history.keys() if 'acc' in s and 'val' in s]\n",
    "    \n",
    "    if len(loss_list) == 0:\n",
    "        print('Loss is missing in history')\n",
    "        return \n",
    "    \n",
    "    ## As loss always exists\n",
    "    epochs = range(1,len(history.history[loss_list[0]]) + 1)\n",
    "    \n",
    "    ## Loss\n",
    "    plt.figure(1)\n",
    "    for l in loss_list:\n",
    "        plt.plot(epochs, history.history[l], 'b', label='Training loss (' + str(str(format(history.history[l][-1],'.5f'))+')'))\n",
    "    for l in val_loss_list:\n",
    "        plt.plot(epochs, history.history[l], 'g', label='Validation loss (' + str(str(format(history.history[l][-1],'.5f'))+')'))\n",
    "    plt.title('Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##  Plot Raw Predictions\n",
    "\n",
    "Let's use the `compare_predictions` helper function to compare predictions from the model when it was untrained and when it was trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-23T06:16:44.092223Z",
     "start_time": "2022-05-23T06:16:44.077225Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_predictions(preds, y_test):\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.plot(preds, y_test, 'ro')\n",
    "    plt.xlabel('Preds')\n",
    "    plt.ylabel('Labels')\n",
    "    plt.xlim([-0.5, 0.5])\n",
    "    plt.ylim([-0.5, 0.5])\n",
    "    plt.plot([-0.5, 0.5], [-0.5, 0.5], 'b--')\n",
    "    plt.show()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-23T06:16:44.107224Z",
     "start_time": "2022-05-23T06:16:44.093223Z"
    }
   },
   "outputs": [],
   "source": [
    "history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-23T06:16:44.123248Z",
     "start_time": "2022-05-23T06:16:44.108226Z"
    }
   },
   "outputs": [],
   "source": [
    "def compare_predictions(preds1, preds2, y_test):\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.plot(preds1, y_test, 'ro', label='Untrained Model')\n",
    "    plt.plot(preds2, y_test, 'go', label='Trained Model')\n",
    "    plt.xlabel('Preds')\n",
    "    plt.ylabel('Labels')\n",
    "    \n",
    "    y_min = min(min(y_test), min(preds1), min(preds2))\n",
    "    y_max = max(max(y_test), max(preds1), max(preds2))\n",
    "    \n",
    "    plt.xlim([y_min, y_max])\n",
    "    plt.ylim([y_min, y_max])\n",
    "    plt.plot([y_min, y_max], [y_min, y_max], 'b--')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Plot Raw Predictions\n",
    "\n",
    "Let's use the `compare_predictions` helper function to compare predictions from the model when it was untrained and when it was trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-23T06:16:44.299224Z",
     "start_time": "2022-05-23T06:16:44.124226Z"
    }
   },
   "outputs": [],
   "source": [
    "preds_on_trained = model.predict(X_test)\n",
    "compare_predictions(preds_on_untrained, preds_on_trained, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot User salary Predictions\n",
    "\n",
    "The plot for User_salary predictions and raw predictions will look the same with just one difference: The x and y axis scale is changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-23T06:16:44.442252Z",
     "start_time": "2022-05-23T06:16:44.300224Z"
    }
   },
   "outputs": [],
   "source": [
    "User_salary_on_untrained = [convert_label_value(y) for y in preds_on_untrained]\n",
    "User_salary_on_trained = [convert_label_value(y) for y in preds_on_trained]\n",
    "User_salary_y_test = [convert_label_value(y) for y in y_test]\n",
    "\n",
    "compare_predictions(User_salary_on_untrained, User_salary_on_trained, User_salary_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "We pretty much get the same graph, but the ranges now different. You can see the ranges  or something for both predictions and labels \n",
    "You can see that the train model is a lot more same as in its predictions to ground truth compared to the untrained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-23T06:16:44.457253Z",
     "start_time": "2022-05-23T06:16:44.443223Z"
    }
   },
   "outputs": [],
   "source": [
    "# import pickle \n",
    "# # Saving model to disk\n",
    "# pickle.dump(ET_Model, open('dl_model.pkl','wb'))\n",
    "# model=pickle.load(open('model.pkl','rb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
